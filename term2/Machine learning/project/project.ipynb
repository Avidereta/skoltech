{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import grid_search\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import json\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ExtractRecipe():\n",
    "    \"\"\" \n",
    "    Extracts recipe information from JSON.\n",
    "    \"\"\"\n",
    "    def __init__(self, json):\n",
    "        self.recipe_id = self.set_id(json)\n",
    "        self.cuisine = self.set_cuisine(json)\n",
    "        self.ingredients = self.set_ingredients(json)\n",
    "        self.ingredient_count = len(self.ingredients)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"ID: %s\\nCuisine: %s\\nIngredients: %s\\nNumber of Ingredients: %s\" % (self.recipe_id,\n",
    "                                    self.cuisine,', '.join(self.ingredients),self.ingredient_count)\n",
    "    def set_id(self,json):\n",
    "        \"\"\"\n",
    "        sets the recipe id.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return json['id']\n",
    "        except KeyError:\n",
    "            return '-99'\n",
    "        \n",
    "    def set_cuisine(self,json):\n",
    "        \"\"\"\n",
    "        sets the recipe cuisine.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return json['cuisine']    \n",
    "        except KeyError:\n",
    "            return ''\n",
    "        \n",
    "    def set_ingredients(self,json):\n",
    "        \"\"\"\n",
    "        sets the recipe ingredients.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return json['ingredients']\n",
    "        except KeyError:\n",
    "            return []\n",
    "        \n",
    "    def clean_ingredient(self,s):\n",
    "        \"\"\"\n",
    "        returns a cleaned up version of the entered ingredient.\n",
    "        \"\"\"\n",
    "        from re import sub\n",
    "        return sub('[^A-Za-z0-9]+', ' ', s)\n",
    "    \n",
    "    def get_train(self):\n",
    "        \"\"\"\n",
    "        returns a dictionary of data for the training set.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'cuisine':self.cuisine,\n",
    "            'ingredients':', '.join([self.clean_ingredient(x) for x in self.ingredients]),\n",
    "            'ingredient_count':self.ingredient_count\n",
    "        }\n",
    "    \n",
    "    def get_predict(self):\n",
    "        \"\"\"\n",
    "        returns a dictionary of data for predicting recipes.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'id':self.recipe_id,\n",
    "            'ingredients':', '.join([self.clean_ingredient(x) for x in self.ingredients]),\n",
    "            'ingredient_count':self.ingredient_count\n",
    "        }   \n",
    "\n",
    "\n",
    "def loadTrainSet(dir='train.json'):\n",
    "    \"\"\"\n",
    "    Read in JSON to create training set.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from pandas import DataFrame, Series\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    X = DataFrame([ExtractRecipe(x).get_train() for x in json.load(open(dir,'rb'))])\n",
    "    encoder = LabelEncoder()\n",
    "    X['cuisine'] = encoder.fit_transform(X['cuisine'])\n",
    "    return X, encoder\n",
    "\n",
    "def loadTestSet(dir='test.json'):\n",
    "    \"\"\"\n",
    "    Read in JSON to create test set.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from pandas import DataFrame\n",
    "    return DataFrame([ExtractRecipe(x).get_predict() for x in json.load(open(dir,'rb'))])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def make_unique_ingredients_dict(df):\n",
    "    ingredients = dict()\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    i = 0\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        ingredients_list = row[3].split(', ')\n",
    "\n",
    "        for ingredient in ingredients_list:\n",
    "            words = ingredient.split()\n",
    "            stemmed_words = []\n",
    "\n",
    "            for word in words:\n",
    "                stemmed_words.append(stemmer.stem(word.lower()))\n",
    "\n",
    "            stemmed_ingredient = ' '.join(stemmed_words)\n",
    "\n",
    "            if stemmed_ingredient not in ingredients:\n",
    "                ingredients[stemmed_ingredient] = i\n",
    "                i = i + 1\n",
    "    \n",
    "    return ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "def estimate_acc(clf, train_data, train_labels):\n",
    "    \"\"\"Calculate accuracy by CV\"\"\"\n",
    "    \n",
    "    scores = cross_validation.cross_val_score(clf, train_data, train_labels, cv = 5)\n",
    "\n",
    "    print('Accuracy on training set: {0} +/- {1}'.format(scores.mean(), scores.std() * 2))\n",
    "    \n",
    "def write_submission(reasult_dict):\n",
    "    \"\"\"File to upload\"\"\"\n",
    "    writer = csv.writer(open('submission.csv', 'wt'))\n",
    "    writer.writerow(['id','cuisine'])\n",
    "    for key, value in result_dict.items():\n",
    "        writer.writerow([key, value])\n",
    "\n",
    "def make_preprocessed_matrix(df, unique_ingridients):\n",
    "    \"\"\"Stemm ingredients in dataframe and put into X\"\"\"\n",
    "    \n",
    "    X = dok_matrix((df.shape[0], 1 + len(unique_ingridients) ))\n",
    "    \n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    #print df\n",
    "    for dish_number, row in enumerate(df.itertuples()):\n",
    "        #print row[3], '\\n'\n",
    "        ingridients_list = row[3].split(', ')\n",
    "        \n",
    "        for ingridient in ingridients_list:\n",
    "            words = ingridient.split()\n",
    "            stemmed_words = []\n",
    "\n",
    "            for word in words:\n",
    "                stemmed_words.append(stemmer.stem(word.lower()))\n",
    "            #print stemmed_words\n",
    "            stemmed_ingridient = ' '.join(stemmed_words)\n",
    "            \n",
    "            #if stemmed_indridient in unique_ingridients:\n",
    "            ingridient_index = unique_ingridients[stemmed_ingridient]\n",
    "\n",
    "            X[dish_number, ingridient_index] = 1\n",
    "            #print X\n",
    "        \n",
    "        X[dish_number, len(unique_ingridients) - 1] = row[2]\n",
    "        \n",
    "    return X\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data as DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df, encoder = loadTrainSet()\n",
    "test_df = loadTestSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation of X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39774 (39774, 6682)\n"
     ]
    }
   ],
   "source": [
    "unique_ingredients = make_unique_ingredients_dict(train_df)\n",
    "ingredients = train_df['ingredients']\n",
    "\n",
    "X = make_preprocessed_matrix(train_df, unique_ingredients)\n",
    "y = train_df['cuisine']\n",
    "\n",
    "print len(y), X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on smaller data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 (300, 989)\n"
     ]
    }
   ],
   "source": [
    "n = 300\n",
    "df = train_df[0:n]\n",
    "\n",
    "unique_ingredients = make_unique_ingredients_dict(df)\n",
    "\n",
    "ingredients = train_df['ingredients']\n",
    "\n",
    "X = make_preprocessed_matrix(df, unique_ingredients)\n",
    "y = train_df['cuisine'][0:n]\n",
    "print len(y), X.shape\n",
    "\n",
    "#print train_df['ingredients'][0], train_df['ingredients'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.780743265123 +/- 0.0114403347877\n",
      "[ 0.77486089  0.78894759  0.78131695  0.7739491   0.78464178]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = LinearSVC(C = 0.09)\n",
    "estimate_acc(clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'C': [0.1, 1, 10, 15]\n",
    "}\n",
    "\n",
    "clf = LinearSVC()\n",
    "gs = GridSearchCV(clf, parameters, cv=10, scoring='mean_squared_error')\n",
    "gs.fit(X, y)\n",
    "print gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1, 'fit_prior': True}\n",
      "Accuracy on training set: 0.748403970518 +/- 0.00782642342733\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "clf = BernoulliNB()\n",
    "parameters = {\n",
    "    'alpha': [0, 0.1, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'fit_prior': [True, False]\n",
    "    }\n",
    "\n",
    "gs = GridSearchCV(clf, parameters, cv=10, scoring='mean_squared_error')\n",
    "gs.fit(X, y)\n",
    "print gs.best_params_\n",
    "\n",
    "clf = BernoulliNB(alpha = 0.1, fit_prior = True)\n",
    "estimate_acc(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named xgboost",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-8a16682bdff6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'bst.max.depth'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'eta'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'nround'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'objective'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"multi:softmax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'num_class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named xgboost"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X)\n",
    "params = {'bst.max.depth': 25, 'eta': 0.3, 'nround': 200, 'objective': \"multi:softmax\", 'num_class':20}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
