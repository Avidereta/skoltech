{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import grid_search\n",
    "import json\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ExtractRecipe():\n",
    "    \"\"\" \n",
    "    Extracts recipe information from JSON.\n",
    "    \"\"\"\n",
    "    def __init__(self, json):\n",
    "        self.recipe_id = self.set_id(json)\n",
    "        self.cuisine = self.set_cuisine(json)\n",
    "        self.ingredients = self.set_ingredients(json)\n",
    "        self.ingredient_count = len(self.ingredients)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"ID: %s\\nCuisine: %s\\nIngredients: %s\\nNumber of Ingredients: %s\" % (self.recipe_id,\n",
    "                                    self.cuisine,', '.join(self.ingredients),self.ingredient_count)\n",
    "    def set_id(self,json):\n",
    "        \"\"\"\n",
    "        sets the recipe id.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return json['id']\n",
    "        except KeyError:\n",
    "            return '-99'\n",
    "        \n",
    "    def set_cuisine(self,json):\n",
    "        \"\"\"\n",
    "        sets the recipe cuisine.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return json['cuisine']    \n",
    "        except KeyError:\n",
    "            return ''\n",
    "        \n",
    "    def set_ingredients(self,json):\n",
    "        \"\"\"\n",
    "        sets the recipe ingredients.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return json['ingredients']\n",
    "        except KeyError:\n",
    "            return []\n",
    "        \n",
    "    def clean_ingredient(self,s):\n",
    "        \"\"\"\n",
    "        returns a cleaned up version of the entered ingredient.\n",
    "        \"\"\"\n",
    "        from re import sub\n",
    "        return sub('[^A-Za-z0-9]+', ' ', s)\n",
    "    \n",
    "    def get_train(self):\n",
    "        \"\"\"\n",
    "        returns a dictionary of data for the training set.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'cuisine':self.cuisine,\n",
    "            'ingredients':', '.join([self.clean_ingredient(x) for x in self.ingredients]),\n",
    "            'ingredient_count':self.ingredient_count\n",
    "        }\n",
    "    \n",
    "    def get_predict(self):\n",
    "        \"\"\"\n",
    "        returns a dictionary of data for predicting recipes.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'id':self.recipe_id,\n",
    "            'ingredients':', '.join([self.clean_ingredient(x) for x in self.ingredients]),\n",
    "            'ingredient_count':self.ingredient_count\n",
    "        }   \n",
    "\n",
    "\n",
    "def loadTrainSet(dir='train.json'):\n",
    "    \"\"\"\n",
    "    Read in JSON to create training set.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from pandas import DataFrame, Series\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    X = DataFrame([ExtractRecipe(x).get_train() for x in json.load(open(dir,'rb'))])\n",
    "    encoder = LabelEncoder()\n",
    "    X['cuisine'] = encoder.fit_transform(X['cuisine'])\n",
    "    return X, encoder\n",
    "\n",
    "def loadTestSet(dir='test.json'):\n",
    "    \"\"\"\n",
    "    Read in JSON to create test set.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from pandas import DataFrame\n",
    "    return DataFrame([ExtractRecipe(x).get_predict() for x in json.load(open(dir,'rb'))])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from re import sub\n",
    "\n",
    "def make_unique_ingredients_dict(df, trash_words):\n",
    "    ingredients = dict()\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    i = 0\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        ingredients_list = row[3].split(', ')\n",
    "\n",
    "        for ingredient in ingredients_list:\n",
    "            ingredient = sub('[^A-Za-z]+', ' ', ingredient)\n",
    "            words = ingredient.split()\n",
    "            stemmed_words = []\n",
    "\n",
    "            for word in words:\n",
    "                if word not in trash_words:\n",
    "                    stemmed_words.append(stemmer.stem(word.lower()))\n",
    "            \n",
    "            if len(stemmed_words) > 0:\n",
    "                stemmed_ingredient = ' '.join(stemmed_words)\n",
    "\n",
    "                if stemmed_ingredient not in ingredients:\n",
    "                    ingredients[stemmed_ingredient] = i\n",
    "                    i = i + 1\n",
    "    \n",
    "    return ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey one\n"
     ]
    }
   ],
   "source": [
    "a = ['hey', 'one']\n",
    "if len(a) > 0:\n",
    "    print ' '.join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from scipy.sparse import dok_matrix\n",
    "import csv\n",
    "\n",
    "def estimate_acc(clf, train_data, train_labels):\n",
    "    \"\"\"Calculate accuracy by CV\"\"\"\n",
    "    \n",
    "    scores = cross_validation.cross_val_score(clf, train_data, train_labels, cv = 5)\n",
    "\n",
    "    print('Accuracy on training set: {0} +/- {1}'.format(scores.mean(), scores.std() * 2))\n",
    "    \n",
    "def make_dict(res_list):\n",
    "    \n",
    "    res_dict = {}\n",
    "    \n",
    "    for i in range (len(res_list)):\n",
    "        res_dict[ test_df['id'][i]] = res_list[i]\n",
    "    return res_dict\n",
    "    \n",
    "def write_submission(result_dict):\n",
    "    \"\"\"File to upload\"\"\"\n",
    "    writer = csv.writer(open('submission.csv', 'wt'))\n",
    "    writer.writerow(['id','cuisine'])\n",
    "    for key, value in result_dict.items():\n",
    "        writer.writerow([key, value])\n",
    "\n",
    "def make_preprocessed_matrix(df, unique_ingredients, trash_words):\n",
    "    \"\"\"Stemm ingredients in dataframe and put into X\"\"\"\n",
    "    \n",
    "    X = dok_matrix((df.shape[0], 1 + len(unique_ingredients) ))\n",
    "    \n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    #print df\n",
    "    for dish_number, row in enumerate(df.itertuples()):\n",
    "        #print row[3], '\\n'\n",
    "        ingredients_list = row[3].split(', ')\n",
    "        \n",
    "        for ingredient in ingredients_list:\n",
    "            ingredient = sub('[^A-Za-z]+', ' ', ingredient)\n",
    "            words = ingredient.split()\n",
    "            stemmed_words = []\n",
    "\n",
    "            for word in words:\n",
    "                if word not in trash_words:\n",
    "                    stemmed_words.append(stemmer.stem(word.lower()))\n",
    "            #print stemmed_words\n",
    "            if len(stemmed_words) > 0:\n",
    "                stemmed_ingredient = ' '.join(stemmed_words)\n",
    "            \n",
    "                if stemmed_ingredient in unique_ingredients:\n",
    "                    ingredient_index = unique_ingredients[stemmed_ingredient]\n",
    "\n",
    "                    X[dish_number, ingredient_index] = 1\n",
    "            #print X\n",
    "        \n",
    "        X[dish_number, len(unique_ingredients)] = row[2]\n",
    "        \n",
    "    return X\n",
    "\n",
    "def find_ingredients_freq(df, unique_ingredients, trash_words):\n",
    "\n",
    "    frequences = dict()\n",
    "\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        ingredients_list = row[3].split(', ')\n",
    "\n",
    "        for ingredient in ingredients_list:\n",
    "            ingredient = sub('[^A-Za-z]+', ' ', ingredient)\n",
    "            words = ingredient.split()\n",
    "            stemmed_words = []\n",
    "\n",
    "            for word in words:\n",
    "                if word not in trash_words:\n",
    "                    stemmed_words.append(stemmer.stem(word.lower()))\n",
    "                    \n",
    "            if len(stemmed_words) > 0:\n",
    "                stemmed_ingredient = ' '.join(stemmed_words)\n",
    "            \n",
    "                if stemmed_ingredient not in frequences:\n",
    "                    frequences[stemmed_ingredient] = 1\n",
    "                else: \n",
    "                    frequences[stemmed_ingredient] += 1\n",
    "    \n",
    "    return frequences\n",
    "\n",
    "def find_unique_words(df):\n",
    "\n",
    "    all_words = []\n",
    "\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        ingredients_list = row[3].split(', ')\n",
    "\n",
    "        for ingredient in ingredients_list:\n",
    "            ingredient = sub('[^A-Za-z]+', ' ', ingredient)\n",
    "            words = ingredient.split()\n",
    "\n",
    "            for word in words:\n",
    "                all_words.append(word.lower())\n",
    "                \n",
    "    unique, counts = np.unique(np.array(all_words), return_counts=True)\n",
    "    unique_words = {}\n",
    "    \n",
    "    for i in range (len(unique)):\n",
    "        unique_words[unique[i]] = counts[i]\n",
    "    \n",
    "    return unique_words\n",
    "\n",
    "\n",
    "#def delete_most_freq_ingredients(X): \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data as DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df, encoder = loadTrainSet()\n",
    "test_df = loadTestSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find trash words in data. First step after loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_trash_words(unique_words):\n",
    "    trash_words = {}\n",
    "    \n",
    "    k = 0\n",
    "    for i in range (len(unique_words.keys())):\n",
    "        if unique_words.values()[i] == 1:\n",
    "            trash_words[unique_words.keys()[i]] = k\n",
    "            k += 1\n",
    "    \n",
    "    return trash_words\n",
    "\n",
    "a = find_unique_words(train_df)\n",
    "trash_words = make_trash_words(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation of X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39774 (39774, 6351)\n"
     ]
    }
   ],
   "source": [
    "unique_ingredients = make_unique_ingredients_dict(train_df, trash_words)\n",
    "ingredients = train_df['ingredients']\n",
    "\n",
    "X = make_preprocessed_matrix(train_df, unique_ingredients, trash_words)\n",
    "y = train_df['cuisine']\n",
    "\n",
    "print len(y), X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9944, 6351)\n"
     ]
    }
   ],
   "source": [
    "X_test = make_preprocessed_matrix(test_df, unique_ingredients, trash_words)\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#trash_words = {'plain', 'whole', 'veri', 'style', 'all', 'the',\n",
    "#'a', 'of', 'big', 'with', 'full', 'miniatur', 'and'}\n",
    "frequences = find_ingredients_freq(train_df, unique_ingredients, trash_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sorted(unique_ingredients.items(), key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range (len(frequences.keys())):\n",
    "    if len(frequences.keys()[i].split(' ')) > 2:\n",
    "        print frequences.keys()[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range (len(a.keys())):\n",
    "    if a.values()[i] == 1:\n",
    "        print a.keys()[i]\n",
    "#print frequences.keys()[0].split(' ')\n",
    "#print sorted(a.items(), key=lambda x: x[1])       \n",
    "#trash_words = [plain, whole, veri, style, all, the, a, of, big, with, full, miniatur ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on smaller data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 (300, 981)\n"
     ]
    }
   ],
   "source": [
    "n = 300\n",
    "df = train_df[0:n]\n",
    "\n",
    "unique_ingredients = make_unique_ingredients_dict(df, trash_words)\n",
    "\n",
    "ingredients = train_df['ingredients']\n",
    "\n",
    "Xt = make_preprocessed_matrix(df, unique_ingredients, trash_words)\n",
    "yt = train_df['cuisine'][0:n]\n",
    "print len(yt), Xt.shape\n",
    "\n",
    "#print train_df['ingredients'][0], train_df['ingredients'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 2000)\n",
    "ingredints = \n",
    "ngram_vectorizer.fit_transform()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at mean ingredients through classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nmb_classes = len(set(train_df['cuisine']))\n",
    "\n",
    "indexs = {}\n",
    "means_ingredients = []\n",
    "\n",
    "for i in range (nmb_classes):\n",
    "    indexs[i] = []\n",
    "    \n",
    "for i in range (len(train_df['ingredient_count'])):\n",
    "    indexs[train_df['cuisine'][i]].append(i)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range (nmb_classes):\n",
    "     means_ingredients.append(np.mean(train_df['ingredient_count'][indexs[i]])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "features multiplicated by their importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### SVC\n",
    "Todo normarizate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.784357491901 +/- 0.00783326268749\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = LinearSVC(C = 0.09)\n",
    "estimate_acc(clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LinearSVC()\n",
    "parameters = {\n",
    "    'C': [0.05, 0.09, 1, 1.7]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(clf, parameters, cv=3, scoring = 'accuracy')\n",
    "gs.fit(X, y)\n",
    "\n",
    "print gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "\n",
    "parameters = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [1.3, 1.5, 1.7]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(clf, parameters, cv=3, scoring = 'accuracy')\n",
    "gs.fit(X, y)\n",
    "\n",
    "print gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.778449185849 +/- 0.00764526771224\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty = 'l1')\n",
    "estimate_acc(clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.781968682816 +/- 0.00778257043319\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty = 'l2', C = 1.3)\n",
    "estimate_acc(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1, 'fit_prior': True}\n",
      "Accuracy on training set: 0.748403970518 +/- 0.00782642342733\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "clf = BernoulliNB()\n",
    "parameters = {\n",
    "    'alpha': [0, 0.1, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'fit_prior': [True, False]\n",
    "    }\n",
    "\n",
    "gs = GridSearchCV(clf, parameters, cv=10, scoring='mean_squared_error')\n",
    "gs.fit(X, y)\n",
    "print gs.best_params_\n",
    "\n",
    "clf = BernoulliNB(alpha = 0.1, fit_prior = True)\n",
    "estimate_acc(clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Voting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.784357450738 +/- 0.00764575074818\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "\n",
    "clf1 = LogisticRegression(penalty='l2', C=1.5)\n",
    "clf2 = BernoulliNB(alpha=0.153)\n",
    "clf3 = LinearSVC(C=0.093, loss='squared_hinge', penalty='l2', multi_class='ovr')\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('nb', clf2), ('lsvc', clf3)], voting='hard')\n",
    "#eclf = eclf.fit(X, y)\n",
    "estimate_acc(eclf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(penalty='l2', C=1.5)\n",
    "clf2 = BernoulliNB(alpha=0.153)\n",
    "clf3 = LinearSVC(C=0.093, loss='squared_hinge', penalty='l2', multi_class='ovr')\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('nb', clf2), ('lsvc', clf3)], voting='hard')\n",
    "eclf = eclf.fit(X, y)\n",
    "\n",
    "y_test = eclf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = encoder.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "res_dict = make_dict(y_test)\n",
    "\n",
    "write_submission(res_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "#dtrain = xgb.DMatrix(X)\n",
    "#params = {'bst.max.depth': 25, 'eta': 0.3, 'nround': 200, 'objective': \"multi:softmax\", 'num_class':20}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forests\n",
    "\n",
    "find feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.602202368448 +/- 0.00588308033084\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "forest = DecisionTreeClassifier()\n",
    "estimate_acc(forest, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest.fit(X, y)\n",
    "importances = forest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6681\n",
      "6682\n"
     ]
    }
   ],
   "source": [
    "#print sorted(importances)\n",
    "print len(unique_ingredients)\n",
    "print len(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
