{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import grid_search\n",
    "import json\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ExtractRecipe():\n",
    "    \"\"\" \n",
    "    Extracts recipe information from JSON.\n",
    "    \"\"\"\n",
    "    def __init__(self, json):\n",
    "        self.recipe_id = self.set_id(json)\n",
    "        self.cuisine = self.set_cuisine(json)\n",
    "        self.ingredients = self.set_ingredients(json)\n",
    "        self.ingredient_count = len(self.ingredients)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"ID: %s\\nCuisine: %s\\nIngredients: %s\\nNumber of Ingredients: %s\" % (self.recipe_id,\n",
    "                                    self.cuisine,', '.join(self.ingredients),self.ingredient_count)\n",
    "    def set_id(self,json):\n",
    "        \"\"\"\n",
    "        sets the recipe id.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return json['id']\n",
    "        except KeyError:\n",
    "            return '-99'\n",
    "        \n",
    "    def set_cuisine(self,json):\n",
    "        \"\"\"\n",
    "        sets the recipe cuisine.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return json['cuisine']    \n",
    "        except KeyError:\n",
    "            return ''\n",
    "        \n",
    "    def set_ingredients(self,json):\n",
    "        \"\"\"\n",
    "        sets the recipe ingredients.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return json['ingredients']\n",
    "        except KeyError:\n",
    "            return []\n",
    "        \n",
    "    def clean_ingredient(self,s):\n",
    "        \"\"\"\n",
    "        returns a cleaned up version of the entered ingredient.\n",
    "        \"\"\"\n",
    "        from re import sub\n",
    "        return sub('[^A-Za-z0-9]+', ' ', s)\n",
    "    \n",
    "    def get_train(self):\n",
    "        \"\"\"\n",
    "        returns a dictionary of data for the training set.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'cuisine':self.cuisine,\n",
    "            'ingredients':', '.join([self.clean_ingredient(x) for x in self.ingredients]),\n",
    "            'ingredient_count':self.ingredient_count\n",
    "        }\n",
    "    \n",
    "    def get_predict(self):\n",
    "        \"\"\"\n",
    "        returns a dictionary of data for predicting recipes.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'id':self.recipe_id,\n",
    "            'ingredients':', '.join([self.clean_ingredient(x) for x in self.ingredients]),\n",
    "            'ingredient_count':self.ingredient_count\n",
    "        }   \n",
    "\n",
    "\n",
    "def loadTrainSet(dir='train.json'):\n",
    "    \"\"\"\n",
    "    Read in JSON to create training set.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from pandas import DataFrame, Series\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    X = DataFrame([ExtractRecipe(x).get_train() for x in json.load(open(dir,'rb'))])\n",
    "    encoder = LabelEncoder()\n",
    "    X['cuisine'] = encoder.fit_transform(X['cuisine'])\n",
    "    return X, encoder\n",
    "\n",
    "def loadTestSet(dir='test.json'):\n",
    "    \"\"\"\n",
    "    Read in JSON to create test set.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from pandas import DataFrame\n",
    "    return DataFrame([ExtractRecipe(x).get_predict() for x in json.load(open(dir,'rb'))])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from re import sub\n",
    "\n",
    "def make_unique_ingredients_dict(df):\n",
    "    ingredients = dict()\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    i = 0\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        ingredients_list = row[3].split(', ')\n",
    "\n",
    "        for ingredient in ingredients_list:\n",
    "            ingredient = sub('[^A-Za-z]+', '', ingredient)\n",
    "            words = ingredient.split()\n",
    "            stemmed_words = []\n",
    "\n",
    "            for word in words:\n",
    "                stemmed_words.append(stemmer.stem(word.lower()))\n",
    "\n",
    "            stemmed_ingredient = ' '.join(stemmed_words)\n",
    "\n",
    "            if stemmed_ingredient not in ingredients:\n",
    "                ingredients[stemmed_ingredient] = i\n",
    "                i = i + 1\n",
    "    \n",
    "    return ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "def estimate_acc(clf, train_data, train_labels):\n",
    "    \"\"\"Calculate accuracy by CV\"\"\"\n",
    "    \n",
    "    scores = cross_validation.cross_val_score(clf, train_data, train_labels, cv = 5)\n",
    "\n",
    "    print('Accuracy on training set: {0} +/- {1}'.format(scores.mean(), scores.std() * 2))\n",
    "    \n",
    "def write_submission(reasult_dict):\n",
    "    \"\"\"File to upload\"\"\"\n",
    "    writer = csv.writer(open('submission.csv', 'wt'))\n",
    "    writer.writerow(['id','cuisine'])\n",
    "    for key, value in result_dict.items():\n",
    "        writer.writerow([key, value])\n",
    "\n",
    "def make_preprocessed_matrix(df, unique_ingredients, trash_words):\n",
    "    \"\"\"Stemm ingredients in dataframe and put into X\"\"\"\n",
    "    \n",
    "    X = dok_matrix((df.shape[0], 1 + len(unique_ingredients) ))\n",
    "    \n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    #print df\n",
    "    for dish_number, row in enumerate(df.itertuples()):\n",
    "        #print row[3], '\\n'\n",
    "        ingredients_list = row[3].split(', ')\n",
    "        \n",
    "        for ingredient in ingredients_list:\n",
    "            ingredient = sub('[^A-Za-z]+', ' ', ingredient)\n",
    "            words = ingredient.split()\n",
    "            stemmed_words = []\n",
    "\n",
    "            for word in words:\n",
    "                if word not in trash_words:\n",
    "                    stemmed_words.append(stemmer.stem(word.lower()))\n",
    "            #print stemmed_words\n",
    "            stemmed_ingredient = ' '.join(stemmed_words)\n",
    "            \n",
    "            if stemmed_ingredient in unique_ingredients:\n",
    "                ingredient_index = unique_ingredients[stemmed_ingredient]\n",
    "\n",
    "                X[dish_number, ingredient_index] = 1\n",
    "            #print X\n",
    "        \n",
    "        X[dish_number, len(unique_ingredients)] = row[2]\n",
    "        \n",
    "    return X\n",
    "\n",
    "def find_ingredients_freq(df, unique_ingredients, trash_words):\n",
    "\n",
    "    frequences = dict()\n",
    "\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        ingredients_list = row[3].split(', ')\n",
    "\n",
    "        for ingredient in ingredients_list:\n",
    "            ingredient = sub('[^A-Za-z]+', ' ', ingredient)\n",
    "            words = ingredient.split()\n",
    "            stemmed_words = []\n",
    "\n",
    "            for word in words:\n",
    "                if word not in trash_words:\n",
    "                    stemmed_words.append(stemmer.stem(word.lower()))\n",
    "\n",
    "            stemmed_ingredient = ' '.join(stemmed_words)\n",
    "            \n",
    "            if stemmed_ingredient not in frequences:\n",
    "                frequences[stemmed_ingredient] = 1\n",
    "            else: \n",
    "                frequences[stemmed_ingredient] += 1\n",
    "    \n",
    "    return frequences\n",
    "\n",
    "def find_unique_words(df):\n",
    "\n",
    "    all_words = []\n",
    "\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        ingredients_list = row[3].split(', ')\n",
    "\n",
    "        for ingredient in ingredients_list:\n",
    "            ingredient = sub('[^A-Za-z]+', ' ', ingredient)\n",
    "            words = ingredient.split()\n",
    "\n",
    "            for word in words:\n",
    "                all_words.append(word.lower())\n",
    "                \n",
    "    unique, counts = np.unique(np.array(all_words), return_counts=True)\n",
    "    unique_words = {}\n",
    "    \n",
    "    for i in range (len(unique)):\n",
    "        unique_words[unique[i]] = counts[i]\n",
    "    \n",
    "    return unique_words\n",
    "\n",
    "\n",
    "#def delete_most_freq_ingredients(X): \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data as DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df, encoder = loadTrainSet()\n",
    "test_df = loadTestSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find trash words in data. First step after loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_trash_words(unique_words):\n",
    "    trash_words = {}\n",
    "    \n",
    "    k = 0\n",
    "    for i in range (len(unique_words.keys())):\n",
    "        if unique_words.values()[i] == 1:\n",
    "            trash_words[unique_words.keys()[i]] = k\n",
    "            k += 1\n",
    "    \n",
    "    return trash_words\n",
    "\n",
    "a = find_unique_words(train_df)\n",
    "trash_words = make_trash_words(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation of X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39774 (39774, 6637)\n"
     ]
    }
   ],
   "source": [
    "unique_ingredients = make_unique_ingredients_dict(train_df)\n",
    "ingredients = train_df['ingredients']\n",
    "\n",
    "X = make_preprocessed_matrix(train_df, unique_ingredients)\n",
    "y = train_df['cuisine']\n",
    "\n",
    "print len(y), X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#trash_words = {'plain', 'whole', 'veri', 'style', 'all', 'the',\n",
    "#'a', 'of', 'big', 'with', 'full', 'miniatur', 'and'}\n",
    "frequences = find_ingredients_freq(train_df, unique_ingredients, trash_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print sorted(frequences.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range (len(frequences.keys())):\n",
    "    if len(frequences.keys()[i].split(' ')) > 2:\n",
    "        print frequences.keys()[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3014"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyssop\n",
      "fruitcake\n",
      "woods\n",
      "moulard\n",
      "muscovy\n",
      "speck\n",
      "dulong\n",
      "foccacia\n",
      "brill\n",
      "chee\n",
      "tipo\n",
      "crunch\n",
      "jonshonville\n",
      "jamon\n",
      "mezzetta\n",
      "chopmeat\n",
      "seven\n",
      "pangasius\n",
      "meats\n",
      "estancia\n",
      "ploy\n",
      "blueberri\n",
      "ti\n",
      "langoustines\n",
      "mahlab\n",
      "psyllium\n",
      "jujubes\n",
      "india\n",
      "fondant\n",
      "panetini\n",
      "lap\n",
      "kampyo\n",
      "maraschino\n",
      "better\n",
      "muscavado\n",
      "budweiser\n",
      "fil\n",
      "bream\n",
      "arrow\n",
      "alexia\n",
      "velvet\n",
      "stolichnaya\n",
      "boquerones\n",
      "xuxu\n",
      "farmhouse\n",
      "mojo\n",
      "wieners\n",
      "panch\n",
      "ribeye\n",
      "nakano\n",
      "ocean\n",
      "lambic\n",
      "spreadable\n",
      "victoria\n",
      "passover\n",
      "crosswise\n",
      "te\n",
      "shiromiso\n",
      "dickel\n",
      "cumberland\n",
      "chong\n",
      "garbonzo\n",
      "tropic\n",
      "do\n",
      "cappuccino\n",
      "poundcake\n",
      "pocket\n",
      "linguisa\n",
      "bag\n",
      "bai\n",
      "daiya\n",
      "artificial\n",
      "mentaiko\n",
      "honeysuckle\n",
      "eggnog\n",
      "uni\n",
      "gnocchetti\n",
      "beetroot\n",
      "parslei\n",
      "chua\n",
      "dictine\n",
      "lesser\n",
      "taste\n",
      "tangzhong\n",
      "matzos\n",
      "camellia\n",
      "wheel\n",
      "moose\n",
      "elderflower\n",
      "margherita\n",
      "piquin\n",
      "chartreuse\n",
      "poolish\n",
      "cortland\n",
      "arak\n",
      "opo\n",
      "portuguese\n",
      "filipino\n",
      "breader\n",
      "soi\n",
      "fern\n",
      "ornamental\n",
      "ikura\n",
      "beech\n",
      "true\n",
      "mora\n",
      "wensleydale\n",
      "blueberry\n",
      "shin\n",
      "genoise\n",
      "mondavi\n",
      "kha\n",
      "orchid\n",
      "klondike\n",
      "garland\n",
      "ammonium\n",
      "phoran\n",
      "graviera\n",
      "haloumi\n",
      "samphire\n",
      "snip\n",
      "conchiglie\n",
      "laksa\n",
      "membrillo\n",
      "german\n",
      "vadouvan\n",
      "mooli\n",
      "specials\n",
      "haricot\n",
      "cooki\n",
      "bock\n",
      "topside\n",
      "boy\n",
      "baobab\n",
      "vinaigrett\n",
      "bob\n",
      "chateaubriand\n",
      "soybeans\n",
      "peapods\n",
      "blacan\n",
      "creami\n",
      "saki\n",
      "biga\n",
      "osetra\n",
      "licor\n",
      "beluga\n",
      "cai\n",
      "branston\n",
      "jambon\n",
      "amarena\n",
      "chik\n",
      "knoflook\n",
      "pina\n",
      "peppadews\n",
      "kochu\n",
      "mae\n",
      "nondairy\n",
      "st\n",
      "benne\n",
      "granary\n",
      "shishito\n",
      "pointed\n",
      "tonic\n",
      "creations\n",
      "raki\n",
      "chimichurri\n",
      "nama\n",
      "sato\n",
      "olie\n",
      "bigoli\n",
      "winesap\n",
      "romana\n",
      "argo\n",
      "toulouse\n",
      "george\n",
      "lingcod\n",
      "legumes\n",
      "ogura\n",
      "third\n",
      "pro\n",
      "arrabbiata\n",
      "usukuchi\n",
      "pain\n",
      "america\n",
      "liverwurst\n",
      "grilling\n",
      "mince\n",
      "activ\n",
      "sloe\n",
      "romanesco\n",
      "gel\n",
      "niblets\n",
      "tallow\n",
      "masago\n",
      "darjeeling\n",
      "picnic\n",
      "tyson\n",
      "malted\n",
      "amba\n",
      "loofah\n",
      "pop\n",
      "blackened\n",
      "poi\n",
      "pod\n",
      "mark\n",
      "crawford\n",
      "sablefish\n",
      "ferns\n",
      "pat\n",
      "macarons\n",
      "blends\n",
      "tuttorosso\n",
      "arctic\n",
      "saltpeter\n",
      "delicata\n",
      "caponata\n",
      "tapatio\n",
      "burrito\n",
      "fume\n",
      "jarred\n",
      "friselle\n",
      "mantou\n",
      "almondmilk\n",
      "teff\n",
      "dole\n",
      "hurst\n",
      "evapor\n",
      "nuggets\n",
      "crouton\n",
      "sports\n",
      "dasheen\n",
      "chive\n",
      "road\n",
      "pullman\n",
      "jowl\n",
      "chicharron\n",
      "lop\n",
      "poured\n",
      "energy\n",
      "vindaloo\n",
      "burro\n",
      "unsweeten\n",
      "rusk\n",
      "abbamele\n",
      "pekoe\n",
      "comice\n",
      "hijiki\n",
      "an\n",
      "asakusa\n",
      "liquorice\n",
      "gember\n",
      "verjuice\n",
      "menta\n",
      "cauliflowerets\n",
      "conger\n",
      "beaters\n",
      "jagermeister\n",
      "foster\n",
      "spumante\n",
      "verbena\n",
      "vera\n",
      "manouri\n",
      "casa\n",
      "chocolatecovered\n",
      "angus\n",
      "dinosaur\n",
      "fiber\n",
      "tradicional\n",
      "huckleberries\n",
      "loose\n",
      "chipped\n",
      "guacamol\n",
      "longan\n",
      "asti\n",
      "amontillado\n",
      "reposado\n",
      "glac\n",
      "mellow\n",
      "dew\n",
      "kippered\n",
      "ladys\n",
      "gebhardt\n",
      "petits\n",
      "tuong\n",
      "deviled\n",
      "bris\n",
      "pane\n",
      "buttercup\n",
      "bermuda\n",
      "tomate\n",
      "than\n",
      "orgeat\n",
      "maifun\n",
      "hakusai\n",
      "nugget\n",
      "nashi\n",
      "kamut\n",
      "piccolini\n",
      "robert\n",
      "quahog\n",
      "cornstarch\n",
      "basa\n",
      "chachere\n",
      "success\n",
      "cups\n",
      "hots\n",
      "nonhydrogenated\n",
      "nian\n",
      "marble\n",
      "everglades\n",
      "mian\n",
      "honeycomb\n",
      "vegeta\n",
      "moonshine\n",
      "pekin\n",
      "cucuzza\n",
      "dijonnaise\n",
      "cardoons\n",
      "wolfberries\n",
      "kappa\n",
      "hip\n",
      "herbsaint\n",
      "chapatti\n",
      "fluff\n",
      "cavenders\n",
      "hachiya\n",
      "lillet\n",
      "tender\n",
      "bark\n",
      "patties\n",
      "blackcurrant\n",
      "wok\n",
      "aloe\n",
      "sobrasada\n",
      "gao\n",
      "straight\n",
      "century\n",
      "kroger\n",
      "sardi\n",
      "nielsen\n",
      "kippers\n",
      "miswa\n",
      "wholesome\n",
      "primavera\n",
      "cheek\n",
      "grind\n",
      "lobsters\n",
      "pareve\n",
      "crumpet\n",
      "big\n",
      "bayonne\n",
      "spiny\n",
      "scape\n",
      "cones\n",
      "falafel\n",
      "riso\n",
      "arame\n",
      "goma\n",
      "bellpepper\n",
      "ceci\n",
      "block\n",
      "colorado\n",
      "fettuccini\n",
      "longaniza\n",
      "madagascar\n",
      "sections\n",
      "crabapples\n",
      "atlantic\n",
      "habas\n",
      "chai\n",
      "tubettini\n",
      "nettle\n",
      "heath\n",
      "prik\n",
      "shiraz\n",
      "culinary\n",
      "germain\n",
      "wasabe\n",
      "colman\n",
      "assam\n",
      "escalopes\n",
      "unhulled\n",
      "porter\n",
      "nigari\n",
      "makers\n",
      "frankfurters\n",
      "stellette\n",
      "tortelloni\n",
      "chang\n",
      "hoagi\n",
      "masur\n",
      "standing\n",
      "doubl\n",
      "slim\n",
      "hillshire\n",
      "harvest\n",
      "gomashio\n",
      "challenge\n",
      "crocker\n",
      "saba\n",
      "flake\n",
      "rape\n",
      "petrale\n",
      "fiddlehead\n",
      "ketjap\n",
      "colada\n",
      "hake\n",
      "ricard\n",
      "padron\n",
      "kingfish\n",
      "brittle\n",
      "massey\n",
      "superior\n",
      "garbanzos\n",
      "cotto\n",
      "spot\n",
      "chourico\n",
      "ratatouille\n",
      "trimmed\n",
      "betty\n",
      "snaps\n",
      "redcurrant\n",
      "azuki\n",
      "fedelini\n",
      "oriental\n",
      "lettuces\n",
      "poppyseeds\n",
      "cheong\n",
      "qua\n",
      "skippy\n",
      "hierba\n",
      "shred\n",
      "bordelaise\n",
      "evans\n",
      "suckling\n",
      "preshred\n",
      "lambrusco\n",
      "aquavit\n",
      "dangmyun\n",
      "accompaniment\n",
      "tuscan\n",
      "lox\n",
      "chiltep\n",
      "holland\n",
      "azteca\n",
      "multigrain\n",
      "pansies\n",
      "harusame\n",
      "sweeteners\n",
      "violets\n",
      "fowl\n",
      "tupelo\n",
      "braggs\n",
      "neem\n",
      "gravenstein\n",
      "bagel\n",
      "amberjack\n",
      "martha\n",
      "jimmies\n",
      "muscadet\n",
      "brew\n",
      "matsutake\n",
      "bratwurst\n",
      "pinhead\n",
      "sheepshead\n",
      "based\n",
      "hops\n",
      "ngo\n",
      "riblets\n",
      "sangiovese\n",
      "tangelos\n",
      "melissa\n",
      "bear\n",
      "barberries\n",
      "taiwanese\n",
      "freeze\n",
      "dragon\n",
      "guanabana\n",
      "castelvetrano\n",
      "maca\n",
      "torpedo\n",
      "farfalline\n",
      "beverages\n",
      "lecithin\n",
      "zatarain\n",
      "chilcostle\n",
      "tarde\n",
      "genmai\n",
      "woksaus\n",
      "angled\n",
      "despelette\n",
      "multi\n",
      "sucrolose\n",
      "ounc\n",
      "cox\n",
      "madeleine\n",
      "layer\n",
      "neapolitan\n",
      "hogue\n",
      "lambs\n",
      "hubbard\n",
      "absinthe\n",
      "cross\n",
      "calabrese\n",
      "ascorbic\n",
      "mixers\n",
      "souchong\n",
      "belacan\n",
      "smart\n",
      "galliano\n",
      "arepa\n",
      "boursin\n",
      "awase\n",
      "tony\n",
      "hoi\n",
      "barramundi\n",
      "blackpepper\n",
      "milkfat\n",
      "nido\n",
      "pike\n",
      "gravlax\n",
      "morcilla\n",
      "dhaniya\n",
      "scones\n",
      "zatarains\n",
      "toasts\n",
      "calabash\n",
      "val\n",
      "submarine\n",
      "secret\n",
      "imperial\n",
      "cordial\n",
      "kisses\n",
      "chowchow\n",
      "magret\n",
      "varieti\n",
      "granita\n",
      "branca\n",
      "chioggia\n",
      "breadcrumb\n",
      "greekstyl\n",
      "ortega\n"
     ]
    }
   ],
   "source": [
    "for i in range (len(a.keys())):\n",
    "    if a.values()[i] == 1:\n",
    "        print a.keys()[i]\n",
    "#print frequences.keys()[0].split(' ')\n",
    "#print sorted(a.items(), key=lambda x: x[1])       \n",
    "#trash_words = [plain, whole, veri, style, all, the, a, of, big, with, full, miniatur ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on smaller data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 (300, 989)\n"
     ]
    }
   ],
   "source": [
    "n = 300\n",
    "df = train_df[0:n]\n",
    "\n",
    "unique_ingredients = make_unique_ingredients_dict(df)\n",
    "\n",
    "ingredients = train_df['ingredients']\n",
    "\n",
    "Xt = make_preprocessed_matrix(df, unique_ingredients)\n",
    "yt = train_df['cuisine'][0:n]\n",
    "print len(yt), Xt.shape\n",
    "\n",
    "#print train_df['ingredients'][0], train_df['ingredients'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at mean ingredients through classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nmb_classes = len(set(train_df['cuisine']))\n",
    "\n",
    "indexs = {}\n",
    "means_ingredients = []\n",
    "\n",
    "for i in range (nmb_classes):\n",
    "    indexs[i] = []\n",
    "    \n",
    "for i in range (len(train_df['ingredient_count'])):\n",
    "    indexs[train_df['cuisine'][i]].append(i)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range (nmb_classes):\n",
    "     means_ingredients.append(np.mean(train_df['ingredient_count'][indexs[i]])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "features multiplicated by their importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### SVC\n",
    "Todo normarizate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.783729101479 +/- 0.00774332095281\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = LinearSVC(C = 0.09)\n",
    "estimate_acc(clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LinearSVC()\n",
    "parameters = {\n",
    "    'C': [0.05, 0.09, 1, 1.7]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(clf, parameters, cv=3, scoring = 'accuracy')\n",
    "gs.fit(X, y)\n",
    "\n",
    "print gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l2', 'C': 1.3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "\n",
    "parameters = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [1.3, 1.5, 1.7]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(clf, parameters, cv=3, scoring = 'accuracy')\n",
    "gs.fit(X, y)\n",
    "\n",
    "print gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.778449185849 +/- 0.00764526771224\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty = 'l1')\n",
    "estimate_acc(clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.781968682816 +/- 0.00778257043319\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty = 'l2', C = 1.3)\n",
    "estimate_acc(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1, 'fit_prior': True}\n",
      "Accuracy on training set: 0.748403970518 +/- 0.00782642342733\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "clf = BernoulliNB()\n",
    "parameters = {\n",
    "    'alpha': [0, 0.1, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'fit_prior': [True, False]\n",
    "    }\n",
    "\n",
    "gs = GridSearchCV(clf, parameters, cv=10, scoring='mean_squared_error')\n",
    "gs.fit(X, y)\n",
    "print gs.best_params_\n",
    "\n",
    "clf = BernoulliNB(alpha = 0.1, fit_prior = True)\n",
    "estimate_acc(clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "#dtrain = xgb.DMatrix(X)\n",
    "#params = {'bst.max.depth': 25, 'eta': 0.3, 'nround': 200, 'objective': \"multi:softmax\", 'num_class':20}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forests\n",
    "\n",
    "find feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.602202368448 +/- 0.00588308033084\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "forest = DecisionTreeClassifier()\n",
    "estimate_acc(forest, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest.fit(X, y)\n",
    "importances = forest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6681\n",
      "6682\n"
     ]
    }
   ],
   "source": [
    "#print sorted(importances)\n",
    "print len(unique_ingredients)\n",
    "print len(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
