{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/na/home/dmcgarry/envs/ml/bin/python\n",
    "\"\"\"\n",
    "Trains the models for Kaggle's \"What's Cooking\" contest: https://www.kaggle.com/c/whats-cooking\n",
    "\n",
    "__author__ = \"David McGarry\"\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import cPickle as pickle\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectPercentile, f_classif, SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from helper import *\n",
    "\n",
    "#############\n",
    "### Grids ###\n",
    "#############\n",
    "\n",
    "ingred_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(strip_accents='unicode',analyzer=\"char\",preprocessor=stripString)),\n",
    "    ('feat', SelectPercentile(chi2)),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "ingred_grid = {\n",
    "    'tfidf__ngram_range':[(2,6)],\n",
    "    'feat__percentile':[95,90,85],\n",
    "    'model__C':[5]\n",
    "}\n",
    "\n",
    "pipe_glm = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(strip_accents='unicode',\n",
    "    \tanalyzer=\"char\",preprocessor=stripString)),\n",
    "    ('feat', SelectPercentile(chi2)),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "grid_glm = {\n",
    "\t'greek':{\n",
    "\t\t'model__C': [5], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [37]\n",
    "\t},\n",
    "\t'southern_us':{\n",
    "\t\t'model__C': [5], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [57]\n",
    "\t},\n",
    "\t'filipino':{\n",
    "\t\t'model__C': [7], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [65]\n",
    "\t},\n",
    "\t'indian':{\n",
    "\t\t'model__C': [3], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [52]\n",
    "\t},\n",
    "\t'jamaican':{\n",
    "\t\t'model__C': [5], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [99]\n",
    "\t},\n",
    "\t'spanish':{\n",
    "\t\t'model__C': [3], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [88]\n",
    "\t},\n",
    "\t'italian':{\n",
    "\t\t'model__C': [5], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [91]\n",
    "\t},\n",
    "\t'mexican':{\n",
    "\t\t'model__C': [7], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [93]\n",
    "\t},\n",
    "\t'chinese':{\n",
    "\t\t'model__C': [5], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [76]\n",
    "\t},\n",
    "\t'british':{\n",
    "\t\t'model__C': [10], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [75]\n",
    "\t},\n",
    "\t'thai':{\n",
    "\t\t'model__C': [5], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [97]\n",
    "\t},\n",
    "\t'vietnamese':{\n",
    "\t\t'model__C': [3], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [27]\n",
    "\t},\n",
    "\t'cajun_creole':{\n",
    "\t\t'model__C': [5], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [88]\n",
    "\t},\n",
    "\t'brazilian':{\n",
    "\t\t'model__C': [10], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [98]\n",
    "\t},\n",
    "\t'french':{\n",
    "\t\t'model__C': [5], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [46]\n",
    "\t},\n",
    "\t'japanese':{\n",
    "\t\t'model__C': [5], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [97]\n",
    "\t},\n",
    "\t'irish':{\n",
    "\t\t'model__C': [8], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [17,15,13]\n",
    "\t},\n",
    "\t'korean':{\n",
    "\t\t'model__C': [8], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [54]\n",
    "\t},\n",
    "\t'moroccan':{\n",
    "\t\t'model__C': [10], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [55]\n",
    "\t},\n",
    "\t'russian':{\n",
    "\t\t'model__C': [6], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [91]\n",
    "\t}\n",
    "}\n",
    "\n",
    "pipe_xgb = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(strip_accents='unicode',\n",
    "    \ttokenizer=LemmaTokenizer())),\n",
    "    ('model', xgb.XGBClassifier())\n",
    "])\n",
    "grid_xgb = {\n",
    "\t'greek':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.08], \n",
    "\t\t'model__max_depth': [6], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'southern_us':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.10], \n",
    "\t\t'model__max_depth': [16], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'filipino':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.08], \n",
    "\t\t'model__max_depth': [4], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'indian':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.06], \n",
    "\t\t'model__max_depth': [8], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'jamaican':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.04], \n",
    "\t\t'model__max_depth': [14], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'spanish':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.08], \n",
    "\t\t'model__max_depth': [14], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'italian':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.14], \n",
    "\t\t'model__max_depth': [10], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'mexican':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.12], \n",
    "\t\t'model__max_depth': [10], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'chinese':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.10], \n",
    "\t\t'model__max_depth': [6], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'british':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.06], \n",
    "\t\t'model__max_depth': [10], \n",
    "\t\t'model__n_estimators': [400], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'thai':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.14], \n",
    "\t\t'model__max_depth': [10], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'vietnamese':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.04], \n",
    "\t\t'model__max_depth': [10], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'cajun_creole':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.06], \n",
    "\t\t'model__max_depth': [10], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'brazilian':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.04], \n",
    "\t\t'model__max_depth': [10], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'french':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.10], \n",
    "\t\t'model__max_depth': [10], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'japanese':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.08], \n",
    "\t\t'model__max_depth': [8], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'irish':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.06], \n",
    "\t\t'model__max_depth': [14], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'korean':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.10], \n",
    "\t\t'model__max_depth': [6], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'moroccan':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.10], \n",
    "\t\t'model__max_depth': [8], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'russian':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.06], \n",
    "\t\t'model__max_depth': [10], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t}\n",
    "}\n",
    "\n",
    "\n",
    "vars = [u'ingredient_count',0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,\n",
    "        u'pred_text_greek', u'pred_text_southern_us', u'pred_text_filipino',\n",
    "    \tu'pred_text_indian', u'pred_text_jamaican', u'pred_text_spanish', \n",
    "    \tu'pred_text_italian', u'pred_text_mexican', u'pred_text_chinese', \n",
    "    \tu'pred_text_british', u'pred_text_thai', u'pred_text_vietnamese', \n",
    "    \tu'pred_text_cajun_creole', u'pred_text_brazilian', u'pred_text_french', \n",
    "    \tu'pred_text_japanese', u'pred_text_irish', u'pred_text_korean', \n",
    "    \tu'pred_text_moroccan', u'pred_text_russian']\n",
    "\n",
    "\n",
    "pipe_xgb_final = Pipeline([\n",
    "    ('union', FeatureUnion([\n",
    "        ('lsa', Pipeline([\n",
    "            ('var', VarSelect(keys='ingredients')),\n",
    "        \t('tfidf', TfidfVectorizer(\n",
    "        \t\tstrip_accents='unicode',analyzer=\"char\",\n",
    "        \t\tngram_range=(2,6),preprocessor=stripString)),\n",
    "            ('svd', TruncatedSVD())\n",
    "        ])),\n",
    "        ('feat', Pipeline([\n",
    "            ('var', VarSelect(keys=vars))\n",
    "        ]))\n",
    "    ])),\n",
    "    ('scl', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
    "    ('feat', SelectKBest(f_classif)),\n",
    "    ('model',xgb.XGBClassifier())\n",
    "])\n",
    "grid_xgb_final = {\n",
    "\t'union__lsa__svd__n_components':[50],\n",
    "    'feat__k':[85,75],\n",
    "    'model__n_estimators':[750],\n",
    "    'model__learning_rate': [0.08],\n",
    "    'model__max_depth':[16,18],\n",
    "    'model__subsample': [0.65],\n",
    "    'model__nthread':[20]\n",
    "}\n",
    "\n",
    "pipe_rf_final = Pipeline([\n",
    "    ('union', FeatureUnion([\n",
    "        ('lsa', Pipeline([\n",
    "            ('var', VarSelect(keys='ingredients')),\n",
    "        \t('tfidf', TfidfVectorizer(\n",
    "        \t\tstrip_accents='unicode',analyzer=\"char\",\n",
    "        \t\tngram_range=(2,6),tokenizer=LemmaTokenizer())),\n",
    "            ('svd', TruncatedSVD())\n",
    "        ])),\n",
    "        ('feat', Pipeline([\n",
    "            ('var', VarSelect(keys=vars))\n",
    "        ]))\n",
    "    ])),\n",
    "    ('scl', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
    "    ('feat', SelectKBest(f_classif)),\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "grid_rf_final = {\n",
    "\t'union__lsa__svd__n_components':[60,75,90],\n",
    "    'feat__k':[80],\n",
    "    'model__n_estimators':[250,500,750],\n",
    "    'model__max_features':[8],\n",
    "    'model__max_depth':[35],\n",
    "    'model__n_jobs':[20]\n",
    "}\n",
    "\n",
    "############\n",
    "### Main ###\n",
    "############\n",
    "\n",
    "def main():\n",
    "\t# load data\n",
    "\ttrain, encoder = loadTrainSet()\n",
    "\tcv = KFold(train.shape[0], n_folds=8, shuffle=True)\n",
    "\n",
    "\t# train ingredient model\n",
    "\tprint \"Ingredient Model\"\n",
    "\tingred_pred, ingred_model = trainIngredient(ingred_pipe,ingred_grid,train,cv,n_jobs=-1)\n",
    "\ttrain = train.join(pd.DataFrame(ingred_pred))\n",
    "\n",
    "\t# train text models\n",
    "\ttext_models = {}\n",
    "\tfor v in [encoder.inverse_transform(v) for v in train.cuisine.unique()]:\n",
    "\t\tprint \"\\nText Model: %s\" % v\n",
    "\t\ttrain['pred_text_'+v], text_models[v] = trainText(pipe_glm,grid_glm[v],pipe_xgb,grid_xgb[v],train.ingredients,train.cuisine.apply(lambda x: 1 if x == encoder.transform(v) else 0),cv,n_jobs=-1)\n",
    "\n",
    "\t# train feature models\n",
    "\tprint \"\\nFeature Model: xgb\"\n",
    "\txgb_pred, recipe_model_xgb = trainFeatureModel(train,train.cuisine,pipe_xgb_final,grid_xgb_final,cv)\n",
    "\tprint \"\\nFeature Model: rf\"\n",
    "\trf_pred, recipe_model_rf = trainFeatureModel(train,train.cuisine,pipe_rf_final,grid_rf_final,cv)\n",
    "\n",
    "\t# blend feature models into final recipe model\n",
    "\tfinal_model = RecipeModel(ingred_model,text_models,recipe_model_xgb,recipe_model_rf,encoder)\n",
    "\tfinal_model.set_weights(xgb_pred,rf_pred,train.cuisine)\n",
    "\tprint final_model\n",
    "\n",
    "\t#make predictions\n",
    "\ttest = loadTestSet()\n",
    "\ttest['cuisine'] = final_model.predict_kaggle(test)\n",
    "\ttest[['id','cuisine']].to_csv(\"../data/pred.csv\",index=False)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
