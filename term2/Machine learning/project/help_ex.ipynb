{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class ExtractRecipe():\n",
    "    \"\"\" \n",
    "    Class that extracts recipe information from JSON.\n",
    "    \"\"\"\n",
    "    def __init__(self,json):\n",
    "        self.recipe_id = self.set_id(json)\n",
    "        self.cuisine = self.set_cuisine(json)\n",
    "        self.ingredients = self.set_ingredients(json)\n",
    "        self.ingredient_count = len(self.ingredients)\n",
    "    def __str__(self):\n",
    "        return \"ID: %s\\nCuisine: %s\\nIngredients: %s\\nNumber of Ingredients: %s\" % (self.recipe_id, self.cuisine,', '.join(self.ingredients),self.ingredient_count)\n",
    "    def set_id(self,json):\n",
    "        \"\"\"\n",
    "        Method that sets the recipe id.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return json['id']\n",
    "        except KeyError:\n",
    "            return '-99'\n",
    "    def set_cuisine(self,json):\n",
    "        \"\"\"\n",
    "        Method that sets the recipe cuisine.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return json['cuisine']    \n",
    "        except KeyError:\n",
    "            return ''\n",
    "    def set_ingredients(self,json):\n",
    "        \"\"\"\n",
    "        Method that sets the recipe ingredients.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return json['ingredients']\n",
    "        except KeyError:\n",
    "            return []\n",
    "    def clean_ingredient(self,s):\n",
    "    \t\"\"\"\n",
    "    \tMethod that returns a cleaned up version of the entered ingredient.\n",
    "    \t\"\"\"\n",
    "    \tfrom re import sub\n",
    "    \treturn sub('[^A-Za-z0-9]+', ' ', s)\n",
    "    def get_train(self):\n",
    "        \"\"\"\n",
    "        Method that returns a dictionary of data for the training set.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'cuisine':self.cuisine,\n",
    "            'ingredients':', '.join([self.clean_ingredient(x) for x in self.ingredients]),\n",
    "            'ingredient_count':self.ingredient_count\n",
    "        }\n",
    "    def get_predict(self):\n",
    "        \"\"\"\n",
    "        Method that returns a dictionary of data for predicting recipes.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'id':self.recipe_id,\n",
    "            'ingredients':', '.join([self.clean_ingredient(x) for x in self.ingredients]),\n",
    "            'ingredient_count':self.ingredient_count\n",
    "        }   \n",
    "\n",
    "class IngredientModel():\n",
    "\t\"\"\"\n",
    "\tClass that stores an ingredient to cuisine model.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self,model):\n",
    "\t\tself.model = model\n",
    "\tdef predict(self,X):\n",
    "\t\tfrom pandas import Series\n",
    "\t\tfrom operator import add\n",
    "\t\treturn X.ingredients.str.split(',? ').apply(lambda recipe: Series(reduce(add,[self.model.predict_proba([x]) for x in recipe])[0]/len(recipe)))\n",
    "\n",
    "class TextModel():\n",
    "\t\"\"\"\n",
    "\tClass that stores and a simple weighted average of two text-based individual cuisine models.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self,a_model,b_model):\n",
    "\t\tself.a_model = a_model\n",
    "\t\tself.b_model = b_model\n",
    "\t\tself.a_weight = 0.5\n",
    "\t\tself.b_weight = 0.5\n",
    "\tdef set_weights(self,a_weight,b_weight):\n",
    "\t\tself.a_weight = a_weight\n",
    "\t\tself.b_weight = b_weight\n",
    "\tdef blend(self,a_pred,b_pred):\n",
    "\t\treturn a_pred*self.a_weight + b_pred*self.b_weight\n",
    "\tdef predict(self,X):\n",
    "\t\ta_pred = self.a_model.predict_proba(X)[:,1]\n",
    "\t\tb_pred = self.b_model.predict_proba(X)[:,1]\n",
    "\t\treturn self.blend(a_pred,b_pred)\n",
    "\n",
    "class RecipeModel():\n",
    "\t\"\"\"\n",
    "\tClass that stores the models needed to predict the type of cuisine based on a list of ingredients.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self,ingred_model,text_models,recipe_model_a,recipe_model_b,encoder):\n",
    "\t\tself.ingred_model = ingred_model\n",
    "\t\tself.text_models = text_models\n",
    "\t\tself.recipe_model_a = recipe_model_a\n",
    "\t\tself.recipe_model_b = recipe_model_b\n",
    "\t\tself.recipe_weight_a = 0.5\n",
    "\t\tself.recipe_weight_b = 0.5\n",
    "\t\tself.score = 0.0\n",
    "\t\tself.encoder = encoder\n",
    "\tdef __str__(self):\n",
    "\t\treturn \"\\nRecipe Model\\nBlended Accuracy: %0.5f\\nModel A Weight: %0.2f\\nModel B Weight: %0.2f\" % (self.score, self.recipe_weight_a, self.recipe_weight_b)\n",
    "\tdef set_weights(self,pred_a,pred_b,target):\n",
    "\t\tfrom sklearn.metrics import accuracy_score\n",
    "\t\tfor w in zip(range(1,100,1),range(99,0,-1)):\n",
    "\t\t\t\tscore = accuracy_score(target,(pred_a*w[0]/100.0+pred_b*w[1]/100.0).argmax(1))\n",
    "\t\t\t\tif score > self.score:\n",
    "\t\t\t\t\tself.recipe_weight_a = w[0]/100.0\n",
    "\t\t\t\t\tself.recipe_weight_b = w[1]/100.0\n",
    "\t\t\t\t\tself.score = score\n",
    "\tdef predict_kaggle(self,X,prob=False):\n",
    "\t\t# add average ingredient scores for each cuisine\n",
    "\t\tX = X.join(self.ingred_model.predict(X))\n",
    "\t\t# add cuisine based text models\n",
    "\t\tfor v in self.text_models.keys():\n",
    "\t\t\tX['pred_text_'+v] = self.text_models[v].predict(X.ingredients)\n",
    "\t\t# make prediction for recipe model\n",
    "\t\tpred_a = self.recipe_model_a.predict_proba(X)\n",
    "\t\tpred_b = self.recipe_model_b.predict_proba(X)\n",
    "\t\tpred = pred_a*self.recipe_weight_a + pred_b*self.recipe_weight_b\n",
    "\t\tif prob:\n",
    "\t\t\treturn pred\n",
    "\t\telse:\n",
    "\t\t\treturn self.encoder.inverse_transform(pred.argmax(1))\n",
    "\tdef predict(self,json_list,prob=False):\n",
    "\t\t\"\"\"\n",
    "\t\tReturn: The predicted cuisine for the list of recipes. \n",
    "\t\tParams:\n",
    "\t\t\t* json_list (List of Dicts): The list of JSON recipes seeking cuisine predictions. \n",
    "\t\t\t* prob: (Boolean) If the output should be the predicted probability across all cuisines or the best guess label. Defaults to False.\t\n",
    "\t\tDoctest:\n",
    "\t\t>>> json_list = [\n",
    "\t\t...     {\n",
    "\t\t...             'id':1,\n",
    "\t\t...             'ingredients': ['pork, black beans, avocado, orange, cumin, salt, cinnamon']\n",
    "\t\t...     },\n",
    "\t\t...     {\n",
    "\t\t...             'id':2,\n",
    "\t\t...             'ingredients': ['pasta, basil, pine nuts, olive oil, parmesan cheese, garlic']\n",
    "\t\t...     },\n",
    "\t\t...     {\n",
    "\t\t...             'id':3,\n",
    "\t\t...             'ingredients': ['tumeric, red lentils, naan, garam masala, onions, sweet potatoes']\n",
    "\t\t...     }\n",
    "\t\t... ]\n",
    "\t\t>>> recipe_model.predict(json_list)\n",
    "\t\tarray([u'mexican', u'italian', u'indian'], dtype=object)\n",
    "\t\t>>> recipe_model.predict(json_list,prob=True)\n",
    "\t\tarray([[  3.30066887e-03,   1.99567227e-06,   1.69680381e-03,\n",
    "\t\t\t\t  1.05174537e-05,   2.03085874e-05,   3.29112324e-03,\n",
    "\t\t\t\t  1.15989352e-06,   4.94386325e-03,   3.40759867e-06,\n",
    "\t\t\t\t  5.00931910e-03,   1.64480210e-03,   1.65024605e-03,\n",
    "\t\t\t\t  7.54782889e-07,   9.33262747e-01,   5.83895764e-07,\n",
    "\t\t\t\t  3.17332176e-06,   2.34691288e-02,   1.01873500e-02,\n",
    "\t\t\t\t  6.56450009e-03,   4.93754585e-03],\n",
    "\t\t\t   [  1.48577580e-05,   4.19468051e-06,   1.68586413e-03,\n",
    "\t\t\t\t  1.27146558e-05,   8.20049665e-06,   1.16352625e-02,\n",
    "\t\t\t\t  3.69138042e-02,   3.23459014e-05,   1.23802178e-04,\n",
    "\t\t\t\t  5.19810867e-01,   1.16872025e-05,   1.17136206e-04,\n",
    "\t\t\t\t  6.88425371e-06,   4.24320803e-01,   1.23880210e-05,\n",
    "\t\t\t\t  1.07642463e-05,   3.49156883e-03,   1.75572406e-03,\n",
    "\t\t\t\t  2.47755352e-05,   6.35768902e-06],\n",
    "\t\t\t   [  3.28104312e-03,   1.04083038e-05,   1.76803405e-06,\n",
    "\t\t\t\t  1.64068986e-03,   9.84115643e-03,   1.66666236e-03,\n",
    "\t\t\t\t  1.31338270e-02,   6.97641581e-01,   3.29681417e-03,\n",
    "\t\t\t\t  1.31455590e-02,   2.93287824e-06,   7.81646840e-02,\n",
    "\t\t\t\t  3.98645284e-07,   4.43144651e-02,   1.05028641e-01,\n",
    "\t\t\t\t  2.53745611e-06,   2.05663297e-02,   1.67476028e-03,\n",
    "\t\t\t\t  6.58238004e-03,   3.37299002e-06]])\n",
    "\t\t\"\"\"\n",
    "\t\tfrom pandas import DataFrame\n",
    "\t\t# extract features from JSON\n",
    "\t\tX = DataFrame([ExtractRecipe(x).get_predict() for x in json_list])\n",
    "\t\t# add average ingredient scores for each cuisine\n",
    "\t\tX = X.join(self.ingred_model.predict(X))\n",
    "\t\t# add cuisine based text models\n",
    "\t\tfor v in self.text_models.keys():\n",
    "\t\t\tX['pred_text_'+v] = self.text_models[v].predict(X.ingredients)\n",
    "\t\t# make prediction for recipe model\n",
    "\t\tpred_a = self.recipe_model_a.predict_proba(X)\n",
    "\t\tpred_b = self.recipe_model_b.predict_proba(X)\n",
    "\t\tpred = pred_a*self.recipe_weight_a + pred_b*self.recipe_weight_b\n",
    "\t\tif prob:\n",
    "\t\t\treturn pred\n",
    "\t\telse:\n",
    "\t\t\treturn self.encoder.inverse_transform(pred.argmax(1))\n",
    "    \t\n",
    "class VarSelect(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, keys):\n",
    "        self.keys = keys\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, df):\n",
    "        return df[self.keys]\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "\tdef __init__(self):\n",
    "\t\tself.wnl = WordNetLemmatizer()\n",
    "\tdef __call__(self, doc):\n",
    "\t\treturn [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "def stripString(s):\n",
    "\treturn ', '.join([''.join(y.lower() for y in x if y.isalnum()) for x in s.split(',')])\n",
    "\n",
    "def loadTrainSet(dir='../data/train.json'):\n",
    "\t\"\"\"\n",
    "\tRead in JSON to create training set.\n",
    "\t\"\"\"\n",
    "\timport json\n",
    "\tfrom pandas import DataFrame, Series\n",
    "\tfrom sklearn.preprocessing import LabelEncoder\n",
    "\tX = DataFrame([ExtractRecipe(x).get_train() for x in json.load(open(dir,'rb'))])\n",
    "\tencoder = LabelEncoder()\n",
    "\tX['cuisine'] = encoder.fit_transform(X['cuisine'])\n",
    "\treturn X, encoder\n",
    "\n",
    "def loadTestSet(dir='../data/test.json'):\n",
    "\t\"\"\"\n",
    "\tRead in JSON to create test set.\n",
    "\t\"\"\"\n",
    "\timport json\n",
    "\tfrom pandas import DataFrame\n",
    "\treturn DataFrame([ExtractRecipe(x).get_predict() for x in json.load(open(dir,'rb'))])\n",
    "\n",
    "def fitSklearn(X,y,cv,i,model,multi=False):\n",
    "\t\"\"\"\n",
    "\tTrain a sklearn pipeline or model -- wrapper to enable parallel CV.\n",
    "\t\"\"\"\n",
    "\ttr = cv[i][0]\n",
    "\tvl = cv[i][1]\n",
    "\tmodel.fit(X.iloc[tr],y.iloc[tr])\n",
    "\tif multi:\n",
    "\t\treturn  {\"pred\": model.predict_proba(X.iloc[vl]), \"index\":vl}\n",
    "\telse:\n",
    "\t\treturn  {\"pred\": model.predict_proba(X.iloc[vl])[:,1], \"index\":vl}\n",
    "\n",
    "def trainSklearn(model,grid,train,target,cv,refit=True,n_jobs=5,multi=False):\n",
    "\t\"\"\"\n",
    "\tTrain a sklearn pipeline or model using textual data as input.\n",
    "\t\"\"\"\n",
    "\tfrom joblib import Parallel, delayed   \n",
    "\tfrom sklearn.grid_search import ParameterGrid\n",
    "\tfrom numpy import zeros\n",
    "\tif multi:\n",
    "\t\tpred = zeros((train.shape[0],target.unique().shape[0]))\n",
    "\t\tfrom sklearn.metrics import accuracy_score\n",
    "\t\tscore_func = accuracy_score\n",
    "\telse:\n",
    "\t\tfrom sklearn.metrics import roc_auc_score\n",
    "\t\tscore_func = roc_auc_score\n",
    "\t\tpred = zeros(train.shape[0])\n",
    "\tbest_score = 0\n",
    "\tfor g in ParameterGrid(grid):\n",
    "\t\tmodel.set_params(**g)\n",
    "\t\tif len([True for x in g.keys() if x.find('nthread') != -1 ]) > 0:\n",
    "\t\t\tresults = [fitSklearn(train,target,list(cv),i,model,multi) for i in range(cv.n_folds)]\n",
    "\t\telse:\n",
    "\t\t\tresults = Parallel(n_jobs=n_jobs)(delayed(fitSklearn)(train,target,list(cv),i,model,multi) for i in range(cv.n_folds))\n",
    "\t\tif multi:\n",
    "\t\t\tfor i in results:\n",
    "\t\t\t\tpred[i['index'],:] = i['pred']\n",
    "\t\t\tscore = score_func(target,pred.argmax(1))\n",
    "\t\telse:\n",
    "\t\t\tfor i in results:\n",
    "\t\t\t\tpred[i['index']] = i['pred']\n",
    "\t\t\tscore = score_func(target,pred)\n",
    "\t\tif score > best_score:\n",
    "\t\t\tbest_score = score\n",
    "\t\t\tbest_pred = pred.copy()\n",
    "\t\t\tbest_grid = g\n",
    "\tprint \"Best Score: %0.5f\" % best_score \n",
    "\tprint \"Best Grid\", best_grid\n",
    "\tif refit:\n",
    "\t\tmodel.set_params(**best_grid)\n",
    "\t\tmodel.fit(train,target)\n",
    "\treturn best_pred, model\n",
    "\n",
    "def trainText(model_a,modelGrid_a,model_b,modelGrid_b,train,target,cv,refit=True,n_jobs=5):\n",
    "\t\"\"\"\n",
    "\tTrain and blend two univariate text models.\n",
    "\t\"\"\"\n",
    "\tfrom sklearn.metrics import roc_auc_score\n",
    "\tfrom copy import deepcopy\n",
    "\tpred_a, model_a = trainSklearn(deepcopy(model_a),modelGrid_a,train,target,cv,refit=refit,n_jobs=n_jobs)\n",
    "\tpred_b, model_b = trainSklearn(deepcopy(model_b),modelGrid_b,train,target,cv,refit=refit,n_jobs=n_jobs)\n",
    "\tmodels = TextModel(model_a,model_b)\n",
    "\tbest_score = 0\n",
    "\tfor w in zip(range(2,100,2),range(98,0,-2)):\n",
    "\t\tscore = roc_auc_score(target,pred_a*w[0]/100.0+pred_b*w[1]/100.0)\n",
    "\t\tif score > best_score:\n",
    "\t\t\tbest_score = score\n",
    "\t\t\tmodels.set_weights(w[0]/100.0,w[1]/100.0)\n",
    "\tfinal_pred = models.blend(pred_a,pred_b)\n",
    "\tprint \"A Weight:\",models.a_weight\n",
    "\tprint \"B Weight:\", models.b_weight\n",
    "\tprint \"Best Blended Score: %0.5f\" % roc_auc_score(target,final_pred)\n",
    "\treturn final_pred, models\n",
    "\n",
    "def splitIngredients(X):\n",
    "\tfrom pandas import Series\n",
    "\tX2 = X.ingredients.str.split(',? ').apply(lambda x: Series(x)).stack().reset_index(level=1, drop=True)\n",
    "\tX2.name = 'ingredient'\n",
    "\treturn X[['cuisine']].join(X2)\n",
    "\n",
    "def fitIngredients(X,cv,i,model):\n",
    "\t\"\"\"\n",
    "\tTrain a sklearn pipeline or model -- wrapper to enable parallel CV.\n",
    "\t\"\"\"\n",
    "\tfrom operator import add\n",
    "\tfrom pandas import Series\n",
    "\ttr = cv[i][0]\n",
    "\tvl = cv[i][1]\n",
    "\tX2 = splitIngredients(X.iloc[tr])\n",
    "\tmodel.fit(X2.ingredient,X2.cuisine)\n",
    "\treturn  {\"pred\":X.iloc[vl].ingredients.str.split(',? ').apply(lambda recipe:  Series(reduce(add,[model.predict_proba([x]) for x in recipe])[0]/len(recipe))), \"index\":vl}\n",
    "\n",
    "def trainIngredient(model,grid,train,cv,refit=True,n_jobs=5):\n",
    "\tfrom joblib import Parallel, delayed   \n",
    "\tfrom sklearn.grid_search import ParameterGrid\n",
    "\tfrom numpy import zeros\n",
    "\tfrom sklearn.metrics import accuracy_score\n",
    "\tpred = zeros((train.shape[0],train.cuisine.unique().shape[0]))\n",
    "\tbest_score = 0\n",
    "\tfor g in ParameterGrid(grid):\n",
    "\t\tmodel.set_params(**g)\n",
    "\t\tresults = Parallel(n_jobs=n_jobs)(delayed(fitIngredients)(train,list(cv),i,model) for i in range(cv.n_folds))\n",
    "\t\tfor i in results:\n",
    "\t\t\tpred[i['index'],:] = i['pred']\n",
    "\t\tscore = accuracy_score(train.cuisine,pred.argmax(1))\n",
    "\t\tif score > best_score:\n",
    "\t\t\tbest_score = score\n",
    "\t\t\tbest_pred = pred.copy()\n",
    "\t\t\tbest_grid = g\n",
    "\tprint \"Best Score: %0.5f\" % best_score \n",
    "\tprint \"Best Grid\", best_grid\n",
    "\tif refit:\n",
    "\t\tX2 = splitIngredients(train)\n",
    "\t\tmodel.set_params(**best_grid)\n",
    "\t\tmodel.fit(X2.ingredient,X2.cuisine)\n",
    "\treturn best_pred, IngredientModel(model)\n",
    "\t\n",
    "def trainFeatureModel(train,target,model,grid,cv,n_jobs=-1):\n",
    "\tfrom sklearn.grid_search import ParameterGrid\n",
    "\tfrom sklearn.metrics import accuracy_score\n",
    "\tfrom joblib import Parallel, delayed  \n",
    "\tfrom numpy import zeros\n",
    "\tpred = zeros((train.shape[0],target.unique().shape[0]))\n",
    "\tbest_score = 0\n",
    "\tbest_grid = {}\n",
    "\tfor g in ParameterGrid(grid):\n",
    "\t\tmodel.set_params(**g)\n",
    "\t\tif len([True for x in g.keys() if x.find('nthread') != -1 or x.find('n_jobs') != -1 ]) > 0:\n",
    "\t\t\tresults = [fitSklearn(train,target,list(cv),i,model,True) for i in range(cv.n_folds)]\n",
    "\t\telse:\n",
    "\t\t\tresults = Parallel(n_jobs=n_jobs)(delayed(fitSklearn)(train,target,list(cv),i,model,True) for i in range(cv.n_folds))\n",
    "\t\tfor i in results:\n",
    "\t\t\tpred[i['index'],:] = i['pred']\n",
    "\t\tscore = accuracy_score(target,pred.argmax(1))\n",
    "\t\tif score > best_score:\n",
    "\t\t\tbest_score = score\n",
    "\t\t\tbest_pred = pred.copy()\n",
    "\t\t\tbest_grid = g\n",
    "\tprint \"Best Score: %0.5f\" % best_score \n",
    "\tprint \"Best Grid:\", best_grid\n",
    "\tmodel.set_params(**best_grid)\n",
    "\tmodel.fit(train,target)\n",
    "\treturn best_pred, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named xgboost",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f61d1d2caff1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named xgboost"
     ]
    }
   ],
   "source": [
    "#!/na/home/dmcgarry/envs/ml/bin/python\n",
    "\"\"\"\n",
    "Trains the models for Kaggle's \"What's Cooking\" contest: https://www.kaggle.com/c/whats-cooking\n",
    "\n",
    "__author__ = \"David McGarry\"\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import cPickle as pickle\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectPercentile, f_classif, SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from helper import *\n",
    "\n",
    "#############\n",
    "### Grids ###\n",
    "#############\n",
    "\n",
    "ingred_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(strip_accents='unicode',analyzer=\"char\",preprocessor=stripString)),\n",
    "    ('feat', SelectPercentile(chi2)),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "ingred_grid = {\n",
    "    'tfidf__ngram_range':[(2,6)],\n",
    "    'feat__percentile':[95,90,85],\n",
    "    'model__C':[5]\n",
    "}\n",
    "\n",
    "pipe_glm = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(strip_accents='unicode',\n",
    "    \tanalyzer=\"char\",preprocessor=stripString)),\n",
    "    ('feat', SelectPercentile(chi2)),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "grid_glm = {\n",
    "\t'greek':{\n",
    "\t\t'model__C': [5], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [37]\n",
    "\t},\n",
    "\t'southern_us':{\n",
    "\t\t'model__C': [5], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [57]\n",
    "\t},\n",
    "\t'filipino':{\n",
    "\t\t'model__C': [7], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [65]\n",
    "\t},\n",
    "\t'indian':{\n",
    "\t\t'model__C': [3], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [52]\n",
    "\t},\n",
    "\t'jamaican':{\n",
    "\t\t'model__C': [5], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [99]\n",
    "\t},\n",
    "\t'spanish':{\n",
    "\t\t'model__C': [3], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [88]\n",
    "\t},\n",
    "\t'italian':{\n",
    "\t\t'model__C': [5], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [91]\n",
    "\t},\n",
    "\t'mexican':{\n",
    "\t\t'model__C': [7], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [93]\n",
    "\t},\n",
    "\t'chinese':{\n",
    "\t\t'model__C': [5], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [76]\n",
    "\t},\n",
    "\t'british':{\n",
    "\t\t'model__C': [10], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [75]\n",
    "\t},\n",
    "\t'thai':{\n",
    "\t\t'model__C': [5], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [97]\n",
    "\t},\n",
    "\t'vietnamese':{\n",
    "\t\t'model__C': [3], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [27]\n",
    "\t},\n",
    "\t'cajun_creole':{\n",
    "\t\t'model__C': [5], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [88]\n",
    "\t},\n",
    "\t'brazilian':{\n",
    "\t\t'model__C': [10], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [98]\n",
    "\t},\n",
    "\t'french':{\n",
    "\t\t'model__C': [5], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [46]\n",
    "\t},\n",
    "\t'japanese':{\n",
    "\t\t'model__C': [5], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [97]\n",
    "\t},\n",
    "\t'irish':{\n",
    "\t\t'model__C': [8], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [17,15,13]\n",
    "\t},\n",
    "\t'korean':{\n",
    "\t\t'model__C': [8], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [54]\n",
    "\t},\n",
    "\t'moroccan':{\n",
    "\t\t'model__C': [10], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [55]\n",
    "\t},\n",
    "\t'russian':{\n",
    "\t\t'model__C': [6], \n",
    "\t\t'tfidf__ngram_range': [(2, 6)], \n",
    "\t\t'feat__percentile': [91]\n",
    "\t}\n",
    "}\n",
    "\n",
    "pipe_xgb = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(strip_accents='unicode',\n",
    "    \ttokenizer=LemmaTokenizer())),\n",
    "    ('model', xgb.XGBClassifier())\n",
    "])\n",
    "grid_xgb = {\n",
    "\t'greek':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.08], \n",
    "\t\t'model__max_depth': [6], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'southern_us':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.10], \n",
    "\t\t'model__max_depth': [16], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'filipino':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.08], \n",
    "\t\t'model__max_depth': [4], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'indian':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.06], \n",
    "\t\t'model__max_depth': [8], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'jamaican':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.04], \n",
    "\t\t'model__max_depth': [14], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'spanish':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.08], \n",
    "\t\t'model__max_depth': [14], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'italian':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.14], \n",
    "\t\t'model__max_depth': [10], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'mexican':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.12], \n",
    "\t\t'model__max_depth': [10], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'chinese':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.10], \n",
    "\t\t'model__max_depth': [6], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'british':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.06], \n",
    "\t\t'model__max_depth': [10], \n",
    "\t\t'model__n_estimators': [400], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'thai':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.14], \n",
    "\t\t'model__max_depth': [10], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'vietnamese':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.04], \n",
    "\t\t'model__max_depth': [10], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'cajun_creole':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.06], \n",
    "\t\t'model__max_depth': [10], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'brazilian':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.04], \n",
    "\t\t'model__max_depth': [10], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'french':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.10], \n",
    "\t\t'model__max_depth': [10], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'japanese':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.08], \n",
    "\t\t'model__max_depth': [8], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'irish':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.06], \n",
    "\t\t'model__max_depth': [14], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'korean':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.10], \n",
    "\t\t'model__max_depth': [6], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'moroccan':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.10], \n",
    "\t\t'model__max_depth': [8], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t},\n",
    "\t'russian':{\n",
    "\t\t'tfidf__ngram_range': [(1, 2)], \n",
    "\t\t'model__nthread': [20], \n",
    "\t\t'model__learning_rate': [0.06], \n",
    "\t\t'model__max_depth': [10], \n",
    "\t\t'model__n_estimators': [500], \n",
    "\t\t'model__subsample': [0.6]\n",
    "\t}\n",
    "}\n",
    "\n",
    "\n",
    "vars = [u'ingredient_count',0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,\n",
    "        u'pred_text_greek', u'pred_text_southern_us', u'pred_text_filipino',\n",
    "    \tu'pred_text_indian', u'pred_text_jamaican', u'pred_text_spanish', \n",
    "    \tu'pred_text_italian', u'pred_text_mexican', u'pred_text_chinese', \n",
    "    \tu'pred_text_british', u'pred_text_thai', u'pred_text_vietnamese', \n",
    "    \tu'pred_text_cajun_creole', u'pred_text_brazilian', u'pred_text_french', \n",
    "    \tu'pred_text_japanese', u'pred_text_irish', u'pred_text_korean', \n",
    "    \tu'pred_text_moroccan', u'pred_text_russian']\n",
    "\n",
    "\n",
    "pipe_xgb_final = Pipeline([\n",
    "    ('union', FeatureUnion([\n",
    "        ('lsa', Pipeline([\n",
    "            ('var', VarSelect(keys='ingredients')),\n",
    "        \t('tfidf', TfidfVectorizer(\n",
    "        \t\tstrip_accents='unicode',analyzer=\"char\",\n",
    "        \t\tngram_range=(2,6),preprocessor=stripString)),\n",
    "            ('svd', TruncatedSVD())\n",
    "        ])),\n",
    "        ('feat', Pipeline([\n",
    "            ('var', VarSelect(keys=vars))\n",
    "        ]))\n",
    "    ])),\n",
    "    ('scl', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
    "    ('feat', SelectKBest(f_classif)),\n",
    "    ('model',xgb.XGBClassifier())\n",
    "])\n",
    "grid_xgb_final = {\n",
    "\t'union__lsa__svd__n_components':[50],\n",
    "    'feat__k':[85,75],\n",
    "    'model__n_estimators':[750],\n",
    "    'model__learning_rate': [0.08],\n",
    "    'model__max_depth':[16,18],\n",
    "    'model__subsample': [0.65],\n",
    "    'model__nthread':[20]\n",
    "}\n",
    "\n",
    "pipe_rf_final = Pipeline([\n",
    "    ('union', FeatureUnion([\n",
    "        ('lsa', Pipeline([\n",
    "            ('var', VarSelect(keys='ingredients')),\n",
    "        \t('tfidf', TfidfVectorizer(\n",
    "        \t\tstrip_accents='unicode',analyzer=\"char\",\n",
    "        \t\tngram_range=(2,6),tokenizer=LemmaTokenizer())),\n",
    "            ('svd', TruncatedSVD())\n",
    "        ])),\n",
    "        ('feat', Pipeline([\n",
    "            ('var', VarSelect(keys=vars))\n",
    "        ]))\n",
    "    ])),\n",
    "    ('scl', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
    "    ('feat', SelectKBest(f_classif)),\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "grid_rf_final = {\n",
    "\t'union__lsa__svd__n_components':[60,75,90],\n",
    "    'feat__k':[80],\n",
    "    'model__n_estimators':[250,500,750],\n",
    "    'model__max_features':[8],\n",
    "    'model__max_depth':[35],\n",
    "    'model__n_jobs':[20]\n",
    "}\n",
    "\n",
    "############\n",
    "### Main ###\n",
    "############\n",
    "\n",
    "def main():\n",
    "\t# load data\n",
    "\ttrain, encoder = loadTrainSet()\n",
    "\tcv = KFold(train.shape[0], n_folds=8, shuffle=True)\n",
    "\n",
    "\t# train ingredient model\n",
    "\tprint \"Ingredient Model\"\n",
    "\tingred_pred, ingred_model = trainIngredient(ingred_pipe,ingred_grid,train,cv,n_jobs=-1)\n",
    "\ttrain = train.join(pd.DataFrame(ingred_pred))\n",
    "\n",
    "\t# train text models\n",
    "\ttext_models = {}\n",
    "\tfor v in [encoder.inverse_transform(v) for v in train.cuisine.unique()]:\n",
    "\t\tprint \"\\nText Model: %s\" % v\n",
    "\t\ttrain['pred_text_'+v], text_models[v] = trainText(pipe_glm,grid_glm[v],pipe_xgb,grid_xgb[v],train.ingredients,train.cuisine.apply(lambda x: 1 if x == encoder.transform(v) else 0),cv,n_jobs=-1)\n",
    "\n",
    "\t# train feature models\n",
    "\tprint \"\\nFeature Model: xgb\"\n",
    "\txgb_pred, recipe_model_xgb = trainFeatureModel(train,train.cuisine,pipe_xgb_final,grid_xgb_final,cv)\n",
    "\tprint \"\\nFeature Model: rf\"\n",
    "\trf_pred, recipe_model_rf = trainFeatureModel(train,train.cuisine,pipe_rf_final,grid_rf_final,cv)\n",
    "\n",
    "\t# blend feature models into final recipe model\n",
    "\tfinal_model = RecipeModel(ingred_model,text_models,recipe_model_xgb,recipe_model_rf,encoder)\n",
    "\tfinal_model.set_weights(xgb_pred,rf_pred,train.cuisine)\n",
    "\tprint final_model\n",
    "\n",
    "\t#make predictions\n",
    "\ttest = loadTestSet()\n",
    "\ttest['cuisine'] = final_model.predict_kaggle(test)\n",
    "\ttest[['id','cuisine']].to_csv(\"../data/pred.csv\",index=False)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
