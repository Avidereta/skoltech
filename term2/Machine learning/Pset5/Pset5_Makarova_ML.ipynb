{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem set 5. Machine Learning, December 2015. \n",
    "#### Makarova Anastasia.\n",
    "\n",
    "### Problem 1\n",
    "### Solution\n",
    "\n",
    "* Kullbackâ€“Leibler divergence\n",
    "$$D_{KL}(p, q) = \\sum\\limits_{x\\in \\mathcal{X}} p(x) \\ln \\frac{p(x)}{q(x)}$$\n",
    "\n",
    "In our case \n",
    "$$D_{KL}(p, q) = \\sum\\limits_{x\\in \\mathcal{X} \\\\ y\\in \\mathcal{Y}} p(x,y) \\ln \\frac{p(x,y)}{p(x)p(y)}$$\n",
    "\n",
    "* Enthropy\n",
    "$$H(X)=-\\sum_{x \\in X}p(x)\\log_2 p(x)$$\n",
    "\n",
    "* Conditional probability function\n",
    "\n",
    "$$p(x,y) = p(x)p(y|x)$$\n",
    "\n",
    "* Conditional Enthropy\n",
    "\n",
    "\\begin{align}\n",
    "H(Y|X)\\ & = \\sum_{x\\in\\mathcal X}\\,p(x)\\,H(Y|X=x) \n",
    " =-\\sum_{x\\in\\mathcal X} p(x)\\sum_{y\\in\\mathcal Y}\\,p(y|x)\\,\\log\\, p(y|x)=\\\\\n",
    "&\n",
    " =-\\sum_{x\\in\\mathcal X, y\\in\\mathcal Y}p(x,y)\\log\\,p(y|x) = \\\\\n",
    " &=-\\sum_{x\\in\\mathcal X, y\\in\\mathcal Y}p(x,y)\\log \\frac {p(x,y)} {p(x)} \n",
    " = \\sum_{x\\in\\mathcal X, y\\in\\mathcal Y}p(x,y)\\log \\frac {p(x)} {p(x,y)}. \\\\\n",
    "\\end{align}\n",
    "\n",
    "* Useful moment\n",
    "\n",
    "$$\\sum_{y \\in \\mathcal Y}p(y)\\log p(y) = \\sum_{x\\in\\mathcal X, y \\in \\mathcal Y}p(x,y)\\log p(y)$$\n",
    "\n",
    "* Mutual Information\n",
    "\n",
    "\\begin{align}\n",
    "H(Y) - H(Y|X)\\ & =  -\\sum_{y \\in Y}p(y)\\log p(y) - \\sum_{x\\in\\mathcal X, y\\in\\mathcal Y}p(x,y)\\log \\frac {p(x)} {p(x,y)} = \\\\\n",
    "& = - \\sum_{x\\in\\mathcal X, y \\in \\mathcal Y}p(x,y)\\log p(y) - \\sum_{x\\in\\mathcal X, y\\in\\mathcal Y}p(x,y)\\log \\frac {p(x)} {p(x,y)} = \\\\\n",
    "& = - \\sum_{x\\in\\mathcal X, y \\in \\mathcal Y}p(x,y)\\log p(y)p(x) + \\sum_{x\\in\\mathcal X, y\\in\\mathcal Y}p(x,y)\\log p(x,y) = \\\\\n",
    "& = \\sum\\limits_{x\\in \\mathcal{X} \\\\ y\\in \\mathcal{Y}} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} = D_{KL}(p, q).\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "### Solution\n",
    "* Kernel function $K(x,z)$ should be a positive definite function (according to Mercer's condition), i.e. for each square-integrable function $g(x)$\n",
    "\n",
    "$$\\iint K(x,z)g(x)g(z)\\,dx dz \\geq 0$$\n",
    "\n",
    "* Kernel \n",
    "\n",
    "$$K(x,z) = e^{-\\gamma||x - z||^2} $$\n",
    "\n",
    "\n",
    "* Let's consider our kernel and Teylor for the last exp:\n",
    "$$e^{-\\gamma||x - z||^2} = e^{-\\gamma||x||^2}e^{-\\gamma||z||^2}e^{2\\gamma <x,z>} = e^{-\\gamma||x||^2}e^{-\\gamma||z||^2} \\sum_{k = 0}^{\\inf}\\frac{(2\\gamma<x,z>)^k}{k!} $$\n",
    "\n",
    "* $K_1(x,z) = e^{2\\gamma <x,z>}$ is a kernel as it is a sum of products of trivial kernels $2\\gamma <x,z>$ .\n",
    "\n",
    "* Let's prove that $K_2(x,z) = e^{-\\gamma||x||^2}e^{-\\gamma||z||^2}$ is also a kernel. According to the definition \n",
    "for each square-integrable function $g(x)$\n",
    "\n",
    "$$\\iint K_2(x,z)g(x)g(z)\\,dx dz \\geq 0$$\n",
    "\n",
    "$$\\iint e^{-\\gamma||x||^2}e^{-\\gamma||z||^2}g(x)g(z)\\,dx dz  = \\iint \\big(e^{-\\gamma||x||^2}g(x)\\big)^2\\,dx^2 \\geq 0$$\n",
    "\n",
    "So, $K_2(x,z)$ is a kernel. \n",
    "\n",
    "* Then, $K(x,z) = K_1(x,z) * K(x,z)$ is a kernel too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "### Solution\n",
    "\n",
    "* $a_i$ - coefficient of learner $h_i(x)$. Then, weight as $(m+1)$ iteration:\n",
    "\n",
    "$$w_I^{m+1} = \\frac{w_i^me^{-a_my_ih^m(x_i)}}{Z_m}$$\n",
    "\n",
    "* $y_ih^m(x_i) = -1$ if $h^m(x_i) \\neq y_i$\n",
    "* $y_ih^m(x_i) = 1$ if $h^m(x_i) =  y_i$\n",
    "\\begin{align}\n",
    "Z_m\\ & =  \\sum_{i}w_i^me^{-a_my_ih^m(x_i)} = \\sum_{i: h^m(x_i) \\neq y_i}w_i^me^{-a_my_ih^m(x_i)} + \\sum_{i: h^m(x_i) = y_i}w_i^me^{-a_my_ih^m(x_i)} =\\\\\n",
    "& = \\sum_{i: h^m(x_i) \\neq y_i}w_i^me^{a_m} + \\sum_{i: h^m(x_i) = y_i}w_i^me^{-a_m} = \\epsilon_me^{a_m} + (1 - \\epsilon_m)e^{-a_m}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "a_m = \\frac{1}{2}\\log\\frac{1-\\epsilon_m}{\\epsilon_m} \\\\\n",
    "\\frac{1-\\epsilon_m}{\\epsilon_m} = e^{2a_m}\n",
    "\\end{align}\n",
    "\n",
    "New weighted error:\n",
    "\\begin{align}\n",
    "\\epsilon = &\\sum_{i: h^m(x_i) \\neq y_i}w_i^{m+1}e^{a_m} = \\sum_{i: h^m(x_i) \\neq y_i} \\frac{w_i^me^{-a_my_ih^m(x_i)}}{Z_m} = \\\\ \n",
    "& = \\sum_{i: h^m(x_i) \\neq y_i}\\frac{w_i^me^{a_m}}{Z_m} = \\frac{\\epsilon_ma_m}{Z_m} = \\\\\n",
    "& = \\frac{\\epsilon_ma_m}{\\epsilon_me^{a_m} + (1 - \\epsilon_m)e^{-a_m}} = \n",
    "\\frac{a_m}{e^{a_m} + (\\frac{1 - \\epsilon_m}{\\epsilon_m})e^{-a_m}} =\\\\\n",
    "& =  \\frac{a_m}{e^{a_m} +  e^{2a_m}e^{-a_m}} = \\frac{1}{2}.\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
